# -*- coding: utf-8 -*-
"""apache-beam-proper-gcp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17uLC7Jeu2LrIMtBHqkBBgCjOnEnjxq5i
"""

# !pip3 install apache-beam

"""

%%shell
PROJECT=iotssc-307920
BUCKET=iotssc-307920
IN_TOPIC=accel-data
OUT_TOPIC=windowed-data

# deploy step
# https://stackoverflow.com/a/64699700/9184658
# https://medium.com/google-developer-experts/trimming-down-the-cost-of-running-google-cloud-dataflow-at-scale-1c796f72c002
python -m apache_beam_proper_gcp \
  --project $PROJECT \
  --input_topic projects/$PROJECT/topics/$IN_TOPIC \
  --output_topic projects/$PROJECT/topics/$OUT_TOPIC \
  --overlap 8 \
  --runner DataflowRunner \
  --disk_size_gb=10 \
  --workerMachineType=n1-standard-1 \
  --enableStreamingEngine \
  --streaming \
  --staging_location gs://$BUCKET/staging \
  --temp_location gs://$BUCKET/temp \
  --template_location gs://$BUCKET/templates/apache_beam_proper_gcp \
  --region us-central1 \
  --usePublicIps=false \
  --noauth_local_webserver # If your browser is on a different machine then exit and re-run this application with the command-line parameter


"""

# import random
# test_items = [{
#   'event': np.random.random_sample() + 1,
#   'timestamp': time.time() + i,
#   } for i in range(40)
# ]




import argparse
import datetime
import logging
import json

import apache_beam as beam
import apache_beam.transforms.window as window
import apache_beam.transforms.trigger as trigger
from apache_beam.options.pipeline_options import PipelineOptions


class PrintFn(beam.DoFn):
    """A DoFn that prints label, element, its window, and its timstamp. """
    #   https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/streaming_wordcount_debugging.py

    def __init__(self, label):
        self.label = label

    def process(
            self,
            element,
            timestamp=beam.DoFn.TimestampParam,
            window=beam.DoFn.WindowParam):
        # Log at INFO level each element processed.
        logging.info('[%s]: %s %s %s', self.label, element, window, timestamp)
        yield element


class AddWindowingInfoFn(beam.DoFn):
    """output tuple of window(key) + element(value)"""

    def process(self, element, window=beam.DoFn.WindowParam):
        yield (window, element)


def get_data_points(tsw):
    # divide by 1000 to get actual g value
    return list(map(lambda w: float(w['message_body'])/1000, tsw))
    # plant['duration'] = durations[plant['duration']]
    # return plant


# def add_timestamp(x):
#     # https://cloud.google.com/dataflow/docs/resources/faq#how_do_i_handle_nameerrors
#     from apache_beam.transforms.window import TimestampedValue
#     return TimestampedValue(x, x['timestamp']) #

# https://cloud.google.com/pubsub/docs/pubsub-dataflow#code_sample
class AddTimestamps(beam.DoFn):
    def process(self, element, publish_time=beam.DoFn.TimestampParam):
        """Processes each incoming windowed element by extracting the Pub/Sub
        message and its publish timestamp into a dictionary. `publish_time`
        defaults to the publish timestamp returned by the Pub/Sub server. It
        is bound to each element by Beam at runtime.
        """
        logging.info(f"got message: {element}")
        yield {
            "message_body": element.decode("utf-8"),
            "publish_time": datetime.datetime.utcfromtimestamp(
                float(publish_time)
            ).strftime("%Y-%m-%d %H:%M:%S.%f"),
        }


# https://beam.apache.org/documentation/transforms/python/aggregation/sample/
# https://cloud.google.com/pubsub/docs/samples/pubsub-to-gcs
def run(input_topic, output_topic, window_size, overlap, pipeline_args=None):
    # `save_main_session` is set to true because some DoFn's rely on
    # globally imported modules.
    pipeline_options = PipelineOptions(
        pipeline_args, streaming=True, save_main_session=True
    )

    with beam.Pipeline(options=pipeline_options) as p:
        (
            p
            | "Read PubSub Messages" >> beam.io.ReadFromPubSub(topic=input_topic)
            | "Add timestamps to messages" >> beam.ParDo(AddTimestamps())
            | 'After AddTimestampFn' >> beam.ParDo(PrintFn('After AddTimestampFn'))
            | 'Window into' >> beam.WindowInto(
                # seconds; offset to start with the earliest data
                window.SlidingWindows(
                    window_size, overlap, -overlap % window_size),
                trigger=trigger.Repeatedly(trigger.AfterCount(window_size)),
                accumulation_mode=trigger.AccumulationMode.ACCUMULATING
            )
            | 'Add Window Info' >> beam.ParDo(AddWindowingInfoFn())
            | 'Group By Window' >> beam.GroupByKey()
            # timestamped window
            | 'Get each window' >> beam.Map(lambda tsw: tsw[1])
            | 'To JSON data only window' >> beam.Map(lambda w: json.dumps(get_data_points(w)).encode("utf-8"))
            | 'After (To JSON data only window)' >> beam.ParDo(PrintFn('To JSON data only window'))
            # | 'Generate predictions' >> beam.Map(get_pred)
            | "Publish windows to PubSub" >> beam.io.WriteToPubSub(topic=output_topic)
        )


if __name__ == '__main__':
    """Build and run the pipeline."""

    logging.getLogger().setLevel(logging.INFO)
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input_topic",
        help="The Cloud Pub/Sub topic to read from.\n"
        '"projects/<PROJECT_NAME>/topics/<TOPIC_NAME>".',
    )
    parser.add_argument(
        "--output_topic",
        help="The Cloud Pub/Sub topic to read from.\n"
        '"projects/<PROJECT_NAME>/topics/<TOPIC_NAME>".',
    )
    parser.add_argument(
        "--window_size",
        type=float,
        default=16,
        help="Window size in seconds.",
    )
    parser.add_argument(
        "--overlap",
        type=float,
        default=6,
        help="Window overlap in data points.",
    )

    known_args, pipeline_args = parser.parse_known_args()

    # items = [random.random() for i in range(5*WINDOW_SIZE)]
    # https://stackoverflow.com/a/56245141/9184658
    # We use the timestamp field to assign element timestamp (this is just to emulate Pub/Sub events in a controlled way).
    # We window the events, use the windowing info as the key, group by key and write the results to the output folder:
    # items = [{'event': event, 'timestamp': time.time() + event} for event in range(5*WINDOW_SIZE)]
    # https://cloud.google.com/dataflow/docs/samples/molecules-walkthrough#phase_4_prediction
    # project_id = "iotssc-307920"
    # input_topic = "accel-data"
    # output_topic = "windowed-data"

    # Additional parameters
    #      Name    |    Value
    # input_topic  | accel-data
    # output_topic | windowed-data
    # overlap      | 8


    run(
        known_args.input_topic,
        known_args.output_topic,
        known_args.window_size,
        known_args.overlap,
        pipeline_args
    )
